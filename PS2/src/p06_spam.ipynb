{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PSET 2, problem 6, spam classification\n",
    "a) Implement code for processing the spam messages into numpy arrays that can be fed into ML models. Complete get_words, create_dictionary, transform_text functions within our provided src/p06_spam.py.  The provided code will then run your functions and save the resulting dictionary into output/p06_dictionary and a sample of the resulting training matrix into output/p06_sample_train_matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "import svm\n",
    "\n",
    "def get_words(message):\n",
    "    \"\"\"Get the normalized list of words from a message string.\n",
    "\n",
    "    This function should split a message into words, normalize them, and return\n",
    "    the resulting list. For splitting, you should split on spaces. For normalization,\n",
    "    you should convert everything to lowercase.\n",
    "\n",
    "    Args:\n",
    "        message: A string containing an SMS message\n",
    "\n",
    "    Returns:\n",
    "       The list of normalized words from the message.\n",
    "    \"\"\"\n",
    "    return message.lower().split(' ')\n",
    "\n",
    "\n",
    "def create_dictionary(messages):\n",
    "    \"\"\"Create a dictionary mapping words to integer indices.\n",
    "\n",
    "    This function should create a dictionary of word to indices using the provided\n",
    "    training messages. Use get_words to process each message. \n",
    "\n",
    "    Rare words are often not useful for modeling. Please only add words to the dictionary\n",
    "    if they occur in at least five messages.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings containing SMS messages\n",
    "\n",
    "    Returns:\n",
    "        A python dict mapping words to integers.\n",
    "    \"\"\"\n",
    "    messages = [get_words(x) for x in messages]\n",
    "    # now they key is to iterate through every word, and counting them, and then when the count eventually reaches 5, we add them \n",
    "    occurrences = {}\n",
    "    word_dict = {} # this is the dict, that we add, once occurrences is 5 \n",
    "    \n",
    "    for message in messages:\n",
    "        message_as_set = set(message)\n",
    "        # for each word in the set, update the status \n",
    "        for word in message_as_set:\n",
    "            if word not in occurrences:\n",
    "                occurrences[word] = 1\n",
    "            else:\n",
    "                # only care about this if not in word_dict already \n",
    "                if word not in word_dict:\n",
    "                    occurrences[word] += 1 \n",
    "                    if occurrences[word] == 5:\n",
    "                        word_dict[word] = len(word_dict)\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def transform_text(messages, word_dictionary):\n",
    "    \"\"\"Transform a list of text messages into a numpy array for further processing.\n",
    "\n",
    "    This function should create a numpy array that contains the number of times each word\n",
    "    appears in each message. Each row in the resulting array should correspond to each \n",
    "    message and each column should correspond to a word.\n",
    "\n",
    "    Use the provided word dictionary to map words to column indices. Ignore words that \n",
    "    are not present in the dictionary. Use get_words to get the words for a message.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings where each string is an SMS message.\n",
    "        word_dictionary: A python dict mapping words to integers.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array marking the words present in each message.\n",
    "    \"\"\"\n",
    "    # basically turn a message into \n",
    "    # first, return a list of indices, corresponding to words that exist, returning -1\n",
    "    messages_indices = [get_words(message) for message in messages]\n",
    "    # now turn them into a mask \n",
    "    # we can map each word to the index, and that index should \"light up\"\n",
    "    # for each sentence, we should create the message index \n",
    "    # sentence to message index \n",
    "    # turn this into a count dictionary\n",
    "    # counter dict maps index to occurrence\n",
    "    # create a matrix for the embeddings\n",
    "    embeddings = np.zeros((len(messages), len(word_dictionary)))\n",
    "    for idx, message in enumerate(messages_indices):\n",
    "        counter_dict = {}\n",
    "        for word in message:\n",
    "            if word in word_dictionary:\n",
    "                if word_dictionary[word] in counter_dict:\n",
    "                    counter_dict[word_dictionary[word]] += 1\n",
    "                else:\n",
    "                    counter_dict[word_dictionary[word]] = 1\n",
    "        # with counter dict, we create an np.zero\n",
    "        # take the index, and update the value to the key \n",
    "        print(counter_dict)\n",
    "        for k, v in counter_dict.items():\n",
    "            embeddings[idx][k] = v\n",
    "\n",
    "    print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 1}\n",
      "{0: 1, 1: 1}\n",
      "{0: 1}\n",
      "{0: 1, 1: 1}\n",
      "{0: 1}\n",
      "{1: 1}\n",
      "{1: 1}\n",
      "[[2. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Hello, my name is Emilia\"\n",
    "test_messages = [\n",
    "    \"Hello my name is Emilia hello\",\n",
    "    \"Hello my name is Subaru\",\n",
    "    \"Hello world\",\n",
    "    \"Hello my name is Acero\",\n",
    "    \"Hello I miss you\",\n",
    "    \"MY dawg\",\n",
    "    \"you are my friend\"\n",
    "]\n",
    "\n",
    "word_dict = create_dictionary(test_messages)\n",
    "# transform text\n",
    "transform_text(test_messages, word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/p06_dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     train_matrix \u001b[38;5;241m=\u001b[39m transform_text(train_messages, dictionary)\n\u001b[1;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/p06_sample_train_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m, train_matrix[:\u001b[38;5;241m100\u001b[39m,:])\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m test_messages, test_labels \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mload_spam_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/ds6_test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m create_dictionary(train_messages)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./output/p06_dictionary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_matrix \u001b[38;5;241m=\u001b[39m transform_text(train_messages, dictionary)\n\u001b[1;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/p06_sample_train_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m, train_matrix[:\u001b[38;5;241m100\u001b[39m,:])\n",
      "File \u001b[0;32m~/personal/ML/CS229/229Assignments/PS2/src/util.py:124\u001b[0m, in \u001b[0;36mwrite_json\u001b[0;34m(filename, value)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_json\u001b[39m(filename, value):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write the provided value as JSON to the given filename\"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    125\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(value, f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/p06_dictionary'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_messages, train_labels = util.load_spam_dataset('../data/ds6_train.tsv')\n",
    "    val_messages, val_labels = util.load_spam_dataset('../data/ds6_val.tsv')\n",
    "    test_messages, test_labels = util.load_spam_dataset('../data/ds6_test.tsv')\n",
    "    \n",
    "    dictionary = create_dictionary(train_messages)\n",
    "\n",
    "    util.write_json('./output/p06_dictionary', dictionary)\n",
    "\n",
    "    train_matrix = transform_text(train_messages, dictionary)\n",
    "\n",
    "    np.savetxt('./output/p06_sample_train_matrix', train_matrix[:100,:])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
